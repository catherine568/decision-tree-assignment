{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__AC7BU_9k98"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THEORY QUESTIONS:\n",
        "\n",
        "\n",
        "1.What is a Decision Tree, and how does it work?\n",
        " - A decision tree is a supervised learning algorithm used for both classification and regression tasks. It's a tree-like structure that makes predictions by recursively splitting data based on feature values, ultimately leading to a final outcome at a leaf node.\n",
        "\n",
        "2.What are impurity measures in Decision Trees?\n",
        " -  Impurity measures in Decision Trees quantify the homogeneity of a set of samples at a node. The goal of the decision tree algorithm is to split the data at each node in a way that reduces impurity, leading to more homogeneous child nodes. Common impurity measures include:\n",
        "\n",
        "Gini Impurity: Measures the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the distribution of labels in the subset. A lower Gini impurity indicates a more homogeneous node.\n",
        "Entropy: Measures the randomness or disorder in a set of samples. A lower entropy indicates less randomness and a more homogeneous node.\n",
        "Classification Error: The simplest impurity measure, it represents the proportion of misclassified samples at a node.\n",
        "The algorithm chooses the split that results in the greatest reduction in impurity.\n",
        "\n",
        "\n",
        "3.What is the mathematical formula for Gini Impurity?\n",
        " - The mathematical formula for Gini Impurity is:\n",
        "\n",
        "$$Gini = 1 - \\sum_{i=1}^{c} p_i^2$$$$Gini = 1 - \\sum_{i=1}^{c} p_i^2$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$Gini$$Gini$ is the Gini impurity for a node.\n",
        "$c$$c$ is the number of classes.\n",
        "$p_i$$p_i$ is the proportion of samples belonging to class $i$$i$ at that node.\n",
        "The Gini impurity ranges from 0 to 1, where 0 represents perfect purity (all samples belong to the same class) and 1 represents maximum impurity (samples are equally distributed across all classes).\n",
        "\n",
        "\n",
        "4.What is the mathematical formula for Entropy?\n",
        " - The mathematical formula for Entropy is:\n",
        "\n",
        "$$Entropy = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$$$Entropy = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$Entropy$$Entropy$ is the Entropy for a node.\n",
        "$c$$c$ is the number of classes.\n",
        "$p_i$$p_i$ is the proportion of samples belonging to class $i$$i$ at that node.\n",
        "Entropy ranges from 0 to 1 (or higher, depending on the base of the logarithm), where 0 represents perfect purity and higher values indicate greater impurity.\n",
        "\n",
        "\n",
        "\n",
        "5.What is Information Gain, and how is it used in Decision Trees?\n",
        " - Information Gain is a measure used in Decision Trees to determine the effectiveness of a split. It quantifies the reduction in entropy or impurity that results from splitting a dataset based on an attribute. The formula for Information Gain is:\n",
        "\n",
        "$$Information\\ Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)$$$$Information\\ Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$Information\\ Gain(S, A)$$Information\\ Gain(S, A)$ is the information gain of splitting dataset $S$$S$ based on attribute $A$$A$.\n",
        "$Entropy(S)$$Entropy(S)$ is the entropy of dataset $S$$S$.\n",
        "$Values(A)$$Values(A)$ is the set of possible values for attribute $A$$A$.\n",
        "$S_v$$S_v$ is the subset of $S$$S$ where attribute $A$$A$ has value $v$$v$.\n",
        "$|S_v|$$|S_v|$ is the number of samples in $S_v$$S_v$.\n",
        "$|S|$$|S|$ is the number of samples in $S$$S$.\n",
        "In Decision Trees, the algorithm calculates the information gain for each potential split and selects the split that results in the highest information gain. This process is repeated recursively until a stopping criterion is met (e.g., all samples in a node belong to the same class, or a maximum depth is reached). By maximizing information gain at each step, the decision tree effectively reduces uncertainty and makes accurate predictions.\n",
        "\n",
        "\n",
        "6.What is the difference between Gini Impurity and Entropy?\n",
        " - Both Gini Impurity and Entropy are measures used in Decision Trees to evaluate the homogeneity of a node and determine the best split. Here's a breakdown of their differences:\n",
        "\n",
        "Gini Impurity:\n",
        "\n",
        "Focus: Measures the probability of misclassifying a randomly chosen sample.\n",
        "Calculation: Based on the proportion of samples belonging to each class.\n",
        "Range: 0 to 1, where 0 is pure and 1 is maximally impure (for a binary classification, 0.5).\n",
        "Computational Cost: Generally computationally less expensive than entropy as it doesn't involve logarithms.\n",
        "Bias: Tends to isolate the most frequent class in its own branch of the tree.\n",
        "Entropy:\n",
        "\n",
        "Focus: Measures the randomness or disorder in the data.\n",
        "Calculation: Based on the logarithm of the proportion of samples belonging to each class.\n",
        "Range: 0 to potentially higher values (depending on the base of the logarithm and the number of classes), where 0 is pure.\n",
        "Computational Cost: More computationally expensive due to the logarithm calculation.\n",
        "Bias: Tends to produce more balanced trees.\n",
        "Key Differences Summarized:\n",
        "\n",
        "Feature\tGini Impurity\tEntropy\n",
        "Measurement\tProbability of misclassification\tDisorder or randomness\n",
        "Calculation\tProportions of classes\tLogarithm of proportions of classes\n",
        "Computational\tLess expensive\tMore expensive\n",
        "Tree Structure\tTends to isolate dominant class\tTends to produce more balanced trees\n",
        "In practice, both measures often yield similar results in terms of tree structure and performance. The choice between them is often a matter of computational efficiency or preference.\n",
        "\n",
        "\n",
        "7.What is the mathematical explanation behind Decision Trees?\n",
        " - The mathematical explanation behind Decision Trees primarily revolves around the concepts of impurity measures, information gain, and recursive partitioning. Here's a breakdown:\n",
        "\n",
        "Impurity Measures (Gini Impurity or Entropy): As we've discussed, these formulas quantify the homogeneity of a set of samples at a node. The goal is to minimize this impurity after splitting the data.\n",
        "\n",
        "Gini Impurity: $Gini = 1 - \\sum_{i=1}^{c} p_i^2$$Gini = 1 - \\sum_{i=1}^{c} p_i^2$\n",
        "Entropy: $Entropy = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$Entropy = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$\n",
        "Information Gain: This measure determines the effectiveness of a split by quantifying the reduction in impurity (either Gini or Entropy) achieved by splitting the data based on a particular attribute. The split that maximizes Information Gain is chosen at each node.\n",
        "\n",
        "$$Information\\ Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)$$$$Information\\ Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)$$\n",
        "(A similar formula applies when using Gini Impurity, replacing Entropy with Gini).\n",
        "Recursive Partitioning: The process of building a Decision Tree is recursive. Starting with the root node (the entire dataset), the algorithm calculates the Information Gain for splitting on each attribute. The attribute that yields the highest Information Gain is chosen as the splitting criterion for that node. The dataset is then partitioned into subsets based on the values of the chosen attribute, creating child nodes. This process is repeated for each child node until a stopping criterion is met.\n",
        "\n",
        "Stopping Criteria: This could be when a node is pure (all samples belong to the same class), when a maximum tree depth is reached, or when the number of samples in a node falls below a certain threshold.\n",
        "In essence, the Decision Tree algorithm uses these mathematical concepts to greedily search for the optimal splits at each node, aiming to create a tree structure that effectively separates the different classes or predicts the target variable with high accuracy. The mathematical formulas provide the quantitative basis for making these splitting decisions.\n",
        "\n",
        "\n",
        "8.What is Pre-Pruning in Decision Trees?\n",
        " - Pre-pruning is a technique used in Decision Trees to stop the tree building process early, before it has fully grown and potentially overfit the training data. Instead of growing the tree to its maximum possible depth and then pruning it back (post-pruning), pre-pruning sets conditions or criteria for stopping the splitting of a node.\n",
        "\n",
        "Common pre-pruning criteria include:\n",
        "\n",
        "Maximum Depth: Limiting the maximum depth of the tree.\n",
        "Minimum Samples Split: Requiring a minimum number of samples in a node for it to be considered for splitting.\n",
        "Minimum Samples Leaf: Requiring a minimum number of samples in a leaf node.\n",
        "Maximum Leaf Nodes: Limiting the total number of leaf nodes in the tree.\n",
        "Impurity Threshold: Stopping the split if the impurity of a node is below a certain threshold.\n",
        "The goal of pre-pruning is to prevent the tree from becoming too complex and capturing noise in the training data, which can lead to poor generalization on unseen data. It can also help to reduce the computational cost of building the tree.\n",
        "\n",
        "\n",
        "9.What is Post-Pruning in Decision Trees?\n",
        " - Post-pruning is a technique used in Decision Trees where the tree is first grown to its full potential depth, allowing it to perfectly fit the training data (which may include capturing noise and overfitting). After the tree is fully grown, the post-pruning process begins, which involves removing or collapsing branches or nodes based on certain criteria.\n",
        "\n",
        "The goal of post-pruning is to simplify the tree and improve its ability to generalize to unseen data by reducing overfitting. Common post-pruning methods include:\n",
        "\n",
        "Reduced Error Pruning: This method uses a separate validation dataset to evaluate the performance of the tree. Nodes are removed if their removal does not increase the error on the validation set.\n",
        "Cost-Complexity Pruning (Weakest Link Pruning): This method considers a cost function that balances the tree's accuracy on the training data with its complexity (number of leaf nodes). It iteratively removes the \"weakest link\" (the split that, if removed, causes the smallest increase in the cost function) until the desired complexity is reached.\n",
        "In essence, post-pruning allows the tree to initially learn all the patterns in the training data, and then it systematically removes the parts that are likely due to noise or overfitting.\n",
        "\n",
        "\n",
        "10.What is the difference between Pre-Pruning and Post-Pruning?\n",
        " -  a summary of the differences between pre-pruning and post-pruning:\n",
        "\n",
        "Feature\tPre-Pruning\tPost-Pruning\n",
        "Timing\tStops tree growth early\tPrunes a fully grown tree\n",
        "Process\tSets stopping criteria during growth\tRemoves branches/nodes after growth\n",
        "Goal\tPrevent overfitting by stopping complexity increase early\tReduce overfitting by simplifying a complex tree\n",
        "Complexity\tResults in a simpler tree initially\tStarts with a complex tree and simplifies it\n",
        "Computational Cost\tGenerally lower\tGenerally higher (builds full tree first)\n",
        "Risk\tMay stop too early and miss important patterns\tMay overfit training data before pruning\n",
        "In essence, pre-pruning is like setting rules before building a house to limit its size, while post-pruning is like building a large house and then deciding which rooms or sections to remove to make it more manageable. Both aim to improve the generalization of the decision tree by preventing overfitting, but they do so at different stages of the tree-building process.\n",
        "\n",
        "\n",
        "11.What is a Decision Tree Regressor?\n",
        " - A Decision Tree Regressor is a supervised learning algorithm used for regression tasks. Similar to a Decision Tree Classifier, it builds a tree-like structure by recursively splitting the data based on feature values. However, instead of predicting a class label at the leaf nodes, a Decision Tree Regressor predicts a continuous numerical value.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "Splitting: The tree is built by finding the best splits in the data at each node. For regression, the \"best\" split is typically determined by minimizing a measure of impurity or error, such as Mean Squared Error (MSE) or Mean Absolute Error (MAE). The algorithm calculates the impurity/error for different possible splits and chooses the one that results in the greatest reduction in impurity/error.\n",
        "Leaf Nodes: Once the tree is built, the leaf nodes represent the final predictions. For a given data point, it traverses the tree based on its feature values until it reaches a leaf node. The predicted value for that data point is the average (or sometimes median) of the target values of all the training samples that fall into that leaf node.\n",
        "In essence, a Decision Tree Regressor partitions the feature space into a set of rectangular regions, and for any new data point that falls into a particular region, it predicts the average target value of the training data in that region.\n",
        "\n",
        "\n",
        "12.What are the advantages and disadvantages of Decision Trees?\n",
        " - Decision Trees are popular algorithms due to their interpretability and versatility. Here are some of their advantages and disadvantages:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Easy to Understand and Interpret: Decision Trees visually represent decisions, making them easy for humans to follow and understand how predictions are made.\n",
        "Handle Both Numerical and Categorical Data: Decision Trees can work with both types of data without extensive preprocessing like scaling.\n",
        "Non-linear Relationships: They can capture non-linear relationships between features and the target variable.\n",
        "Require Little Data Preparation: Compared to some other algorithms, Decision Trees often require less data cleaning and preparation.\n",
        "Can Handle Missing Values (with some implementations): Some Decision Tree algorithms have mechanisms to handle missing data.\n",
        "Basis for Ensemble Methods: Decision Trees are the fundamental building blocks for powerful ensemble methods like Random Forests and Gradient Boosting.\n",
        "Disadvantages:\n",
        "\n",
        "Prone to Overfitting: Without proper pruning or constraints, Decision Trees can become overly complex and fit the training data too closely, leading to poor performance on unseen data.\n",
        "Instability: Small changes in the data can lead to a completely different tree structure.\n",
        "Greedy Approach: The algorithm makes the best split at each node locally, which doesn't guarantee a globally optimal tree.\n",
        "Bias Towards Features with More Levels: Features with a larger number of unique values can be favored during splitting.\n",
        "Difficulty with Continuous Variables: While they can handle continuous variables, they do so by creating binary splits, which can lose some information compared to methods that use the continuous nature of the data directly.\n",
        "Computational Cost (for large datasets): Building a deep tree can be computationally expensive for very large datasets.\n",
        "Understanding these trade-offs is important when deciding if a Decision Tree is the right algorithm for your specific problem.\n",
        "\n",
        "\n",
        "13.How does a Decision Tree handle missing values?\n",
        " - Handling missing values in Decision Trees can vary depending on the specific implementation and library used. However, here are some common approaches:\n",
        "\n",
        "Ignoring missing values: Some implementations simply ignore instances with missing values for the attribute being considered for a split. When calculating impurity or information gain for a split on a particular feature, only the instances with non-missing values for that feature are used.\n",
        "Assigning to the most frequent category/mean: For categorical features, missing values can be assigned to the most frequent category in the node. For numerical features, missing values might be assigned the mean or median of the non-missing values in the node.\n",
        "Splitting based on non-missing values: Some algorithms can send instances with missing values down all branches of a split, but with reduced weight. The weight assigned to an instance for a particular branch is proportional to the number of non-missing instances in the node that go down that branch.\n",
        "Considering missing as a separate category: For categorical features, missing values can be treated as a distinct category during the splitting process.\n",
        "Using surrogate splits: Some advanced implementations use \"surrogate splits.\" If the best split on a feature has many missing values, the algorithm finds alternative splits on other features that are highly correlated with the original split. Instances with missing values for the primary splitting feature are then directed down the branches of the surrogate split.\n",
        "It's important to consult the documentation of the specific Decision Tree library you are using (like scikit-learn in Python) to understand how it handles missing values by default and what options are available for configuring this behavior.\n",
        "\n",
        "\n",
        "14.How does a Decision Tree handle categorical features?\n",
        " - Handling categorical features in Decision Trees is a fundamental aspect of their operation. Decision Trees work by splitting the data based on feature values to create homogeneous child nodes. For categorical features, this typically involves creating branches for each unique category or grouping categories together. Here's how it generally works:\n",
        "\n",
        "Binary Splits for Multiple Categories: Even if a categorical feature has multiple unique values (e.g., \"Red,\" \"Blue,\" \"Green\"), a Decision Tree typically creates binary splits at each node. This means it will find the best way to divide the categories into two groups to maximize information gain (or minimize impurity). For example, a split might be \"Is the color 'Red'?\" (Yes/No), or \"Is the color in the set {'Blue', 'Green'}?\" (Yes/No).\n",
        "Finding the Best Split: The algorithm evaluates different possible groupings of categories to find the split that results in the highest information gain or lowest impurity. It will consider all possible combinations of splitting the categories into two sets.\n",
        "Handling Ordinal vs. Nominal Categories:\n",
        "Nominal Categories (no inherent order): For nominal features (like color), the tree will simply find the best way to group the categories for binary splits as described above.\n",
        "Ordinal Categories (have an order): For ordinal features (like \"Small,\" \"Medium,\" \"Large\"), the tree can potentially use the inherent order when making splits. For example, a split might be \"Is the size 'Medium' or larger?\" This utilizes the ordered nature of the categories.\n",
        "Libraries and Implementations: The specific details of how categorical features are handled can depend on the Decision Tree library being used. Some libraries might have optimized ways to handle categorical features with many unique values.\n",
        "In essence, Decision Trees handle categorical features by finding the most informative way to split the data based on the categories present in a node, typically resulting in binary branches.\n",
        "\n",
        "\n",
        "\n",
        "15.What are some real-world applications of Decision Trees?\n",
        " - Decision Trees are versatile and have many real-world applications across various domains. Here are some examples:\n",
        "\n",
        "Medical Diagnosis: Decision trees can be used to build models that help diagnose diseases based on patient symptoms, medical history, and test results.\n",
        "Credit Risk Assessment: Financial institutions use decision trees to assess the creditworthiness of loan applicants based on factors like income, credit history, and employment status.\n",
        "Customer Relationship Management (CRM): Decision trees can help identify customer segments, predict customer churn, and personalize marketing campaigns.\n",
        "Fraud Detection: They are used to build models that detect fraudulent transactions in banking, insurance, or other industries by identifying unusual patterns.\n",
        "Spam Filtering: Decision trees can be trained to classify emails as spam or not spam based on the presence of certain keywords or characteristics.\n",
        "Predicting Sales: Businesses can use decision trees to forecast sales based on historical data, promotions, and other relevant factors.\n",
        "Manufacturing and Quality Control: Decision trees can help identify the root causes of defects in manufacturing processes and improve quality control.\n",
        "Image Classification: While deep learning is more prevalent for complex image tasks, decision trees can be used for simpler image classification problems.\n",
        "Bioinformatics: Decision trees can be applied to analyze biological data, such as gene expression levels, to identify patterns and make predictions.\n",
        "These are just a few examples, and the application of Decision Trees is constantly expanding as data becomes more available and computational power increases.\n"
      ],
      "metadata": {
        "id": "GFZi9Lhv-CIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRACTICAL QUESTIONS:"
      ],
      "metadata": {
        "id": "ZcjNOsExEHDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "16.  #Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier object\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F568_3Y3EOeh",
        "outputId": "89a46011-9bf3-4c39-ad94-b1e969939566"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "17.  #Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "# feature importances\n",
        "\n",
        "# Create a Decision Tree Classifier object with Gini Impurity\n",
        "clf_gini = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf_gini = clf_gini.fit(X_train, y_train)\n",
        "\n",
        "# Predict the response for test dataset\n",
        "y_pred_gini = clf_gini.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy (Gini):\", accuracy_score(y_test, y_pred_gini))\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", clf_gini.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKi-CWarEsoi",
        "outputId": "c848d4db-8a32-42e9-a5c6-812e2b2cf1a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Gini): 1.0\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "18. #Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "# model accuracy\n",
        "\n",
        "#Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "#model accuracy\n",
        "\n",
        "# Create a Decision Tree Classifier object with Entropy\n",
        "clf_entropy = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf_entropy = clf_entropy.fit(X_train, y_train)\n",
        "\n",
        "# Predict the response for test dataset\n",
        "y_pred_entropy = clf_entropy.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy (Entropy):\", accuracy_score(y_test, y_pred_entropy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRKIdga8E7Yf",
        "outputId": "1381e11e-f54f-40c5-cf1b-3d08d27c57a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Entropy): 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "19. #Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "# Squared Error (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X_reg = housing.data\n",
        "y_reg = housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor object\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the Decision Tree Regressor\n",
        "regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict the response for test dataset\n",
        "y_pred_reg = regressor.predict(X_test_reg)\n",
        "\n",
        "# Evaluate the model using Mean Squared Error\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "print(\"Mean Squared Error (MSE):\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFAEZk_1FH0r",
        "outputId": "161e7652-73f4-4971-8ec6-c54e948b38d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280096503174904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "20. # Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
        "\n",
        "!pip install graphviz\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Visualize the Decision Tree Classifier trained with Gini impurity\n",
        "dot_data_gini = export_graphviz(clf_gini, out_file=None,\n",
        "                                feature_names=iris.feature_names,\n",
        "                                class_names=iris.target_names,\n",
        "                                filled=True, rounded=True,\n",
        "                                special_characters=True)\n",
        "graph_gini = graphviz.Source(dot_data_gini)\n",
        "# To display the graph, you can save it to a file or render it directly\n",
        "# graph_gini.render(\"iris_gini_tree\")  # Saves to a file named iris_gini_tree.gv and iris_gini_tree.gv.pdf\n",
        "\n",
        "print(\"Decision Tree Classifier (Gini) Visualization:\")\n",
        "display(graph_gini)\n",
        "\n",
        "\n",
        "# Visualize the Decision Tree Classifier trained with Entropy\n",
        "dot_data_entropy = export_graphviz(clf_entropy, out_file=None,\n",
        "                                   feature_names=iris.feature_names,\n",
        "                                   class_names=iris.target_names,\n",
        "                                   filled=True, rounded=True,\n",
        "                                   special_characters=True)\n",
        "graph_entropy = graphviz.Source(dot_data_entropy)\n",
        "# To display the graph, you can save it to a file or render it directly\n",
        "# graph_entropy.render(\"iris_entropy_tree\")  # Saves to a file named iris_entropy_tree.gv and iris_entropy_tree.gv.pdf\n",
        "\n",
        "print(\"\\nDecision Tree Classifier (Entropy) Visualization:\")\n",
        "display(graph_entropy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qX1XSoG2FlLT",
        "outputId": "c1aba220-fbd7-47e1-de76-6a1b7f0dcb41"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.21)\n",
            "Decision Tree Classifier (Gini) Visualization:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"749pt\" height=\"790pt\"\n viewBox=\"0.00 0.00 749.00 790.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 786)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-786 745,-786 745,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M265,-782C265,-782 130,-782 130,-782 124,-782 118,-776 118,-770 118,-770 118,-711 118,-711 118,-705 124,-699 130,-699 130,-699 265,-699 265,-699 271,-699 277,-705 277,-711 277,-711 277,-770 277,-770 277,-776 271,-782 265,-782\"/>\n<text text-anchor=\"start\" x=\"126\" y=\"-766.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n<text text-anchor=\"start\" x=\"162\" y=\"-751.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.664</text>\n<text text-anchor=\"start\" x=\"152.5\" y=\"-736.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 105</text>\n<text text-anchor=\"start\" x=\"139.5\" y=\"-721.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 37, 37]</text>\n<text text-anchor=\"start\" x=\"145\" y=\"-706.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M166,-655.5C166,-655.5 73,-655.5 73,-655.5 67,-655.5 61,-649.5 61,-643.5 61,-643.5 61,-599.5 61,-599.5 61,-593.5 67,-587.5 73,-587.5 73,-587.5 166,-587.5 166,-587.5 172,-587.5 178,-593.5 178,-599.5 178,-599.5 178,-643.5 178,-643.5 178,-649.5 172,-655.5 166,-655.5\"/>\n<text text-anchor=\"start\" x=\"91.5\" y=\"-640.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"78.5\" y=\"-625.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 31</text>\n<text text-anchor=\"start\" x=\"69\" y=\"-610.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 0, 0]</text>\n<text text-anchor=\"start\" x=\"76\" y=\"-595.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M170.44,-698.91C162.93,-687.65 154.78,-675.42 147.24,-664.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"150.07,-662.05 141.61,-655.67 144.25,-665.93 150.07,-662.05\"/>\n<text text-anchor=\"middle\" x=\"136.71\" y=\"-676.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M343,-663C343,-663 208,-663 208,-663 202,-663 196,-657 196,-651 196,-651 196,-592 196,-592 196,-586 202,-580 208,-580 208,-580 343,-580 343,-580 349,-580 355,-586 355,-592 355,-592 355,-651 355,-651 355,-657 349,-663 343,-663\"/>\n<text text-anchor=\"start\" x=\"204\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.75</text>\n<text text-anchor=\"start\" x=\"247.5\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"234.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 74</text>\n<text text-anchor=\"start\" x=\"221\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 37, 37]</text>\n<text text-anchor=\"start\" x=\"223\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M224.56,-698.91C230.43,-690.1 236.7,-680.7 242.76,-671.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"245.85,-673.28 248.49,-663.02 240.03,-669.4 245.85,-673.28\"/>\n<text text-anchor=\"middle\" x=\"253.39\" y=\"-683.84\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#3fe685\" stroke=\"black\" d=\"M252.5,-544C252.5,-544 130.5,-544 130.5,-544 124.5,-544 118.5,-538 118.5,-532 118.5,-532 118.5,-473 118.5,-473 118.5,-467 124.5,-461 130.5,-461 130.5,-461 252.5,-461 252.5,-461 258.5,-461 264.5,-467 264.5,-473 264.5,-473 264.5,-532 264.5,-532 264.5,-538 258.5,-544 252.5,-544\"/>\n<text text-anchor=\"start\" x=\"126.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.6</text>\n<text text-anchor=\"start\" x=\"156\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.059</text>\n<text text-anchor=\"start\" x=\"150.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n<text text-anchor=\"start\" x=\"141\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 32, 1]</text>\n<text text-anchor=\"start\" x=\"139\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M246.36,-579.91C239.97,-571.01 233.15,-561.51 226.56,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"229.27,-550.1 220.59,-544.02 223.58,-554.19 229.27,-550.1\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#9254e9\" stroke=\"black\" d=\"M424.5,-544C424.5,-544 294.5,-544 294.5,-544 288.5,-544 282.5,-538 282.5,-532 282.5,-532 282.5,-473 282.5,-473 282.5,-467 288.5,-461 294.5,-461 294.5,-461 424.5,-461 424.5,-461 430.5,-461 436.5,-467 436.5,-473 436.5,-473 436.5,-532 436.5,-532 436.5,-538 430.5,-544 424.5,-544\"/>\n<text text-anchor=\"start\" x=\"290.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n<text text-anchor=\"start\" x=\"324\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.214</text>\n<text text-anchor=\"start\" x=\"318.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 41</text>\n<text text-anchor=\"start\" x=\"309\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 5, 36]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M304.64,-579.91C311.03,-571.01 317.85,-561.51 324.44,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"327.42,-554.19 330.41,-544.02 321.73,-550.1 327.42,-554.19\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-417.5C109,-417.5 12,-417.5 12,-417.5 6,-417.5 0,-411.5 0,-405.5 0,-405.5 0,-361.5 0,-361.5 0,-355.5 6,-349.5 12,-349.5 12,-349.5 109,-349.5 109,-349.5 115,-349.5 121,-355.5 121,-361.5 121,-361.5 121,-405.5 121,-405.5 121,-411.5 115,-417.5 109,-417.5\"/>\n<text text-anchor=\"start\" x=\"32.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 32</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 32, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M146.05,-460.91C132.83,-449.1 118.4,-436.22 105.23,-424.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107.43,-421.72 97.64,-417.67 102.76,-426.94 107.43,-421.72\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-417.5C240,-417.5 151,-417.5 151,-417.5 145,-417.5 139,-411.5 139,-405.5 139,-405.5 139,-361.5 139,-361.5 139,-355.5 145,-349.5 151,-349.5 151,-349.5 240,-349.5 240,-349.5 246,-349.5 252,-355.5 252,-361.5 252,-361.5 252,-405.5 252,-405.5 252,-411.5 246,-417.5 240,-417.5\"/>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>3&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.89,-460.91C193.25,-450.2 193.65,-438.62 194.02,-427.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"197.52,-427.78 194.37,-417.67 190.53,-427.54 197.52,-427.78\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M424,-425C424,-425 289,-425 289,-425 283,-425 277,-419 277,-413 277,-413 277,-354 277,-354 277,-348 283,-342 289,-342 289,-342 424,-342 424,-342 430,-342 436,-348 436,-354 436,-354 436,-413 436,-413 436,-419 430,-425 424,-425\"/>\n<text text-anchor=\"start\" x=\"285\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n<text text-anchor=\"start\" x=\"328.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"319\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8</text>\n<text text-anchor=\"start\" x=\"309.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 4, 4]</text>\n<text text-anchor=\"start\" x=\"304\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M358.46,-460.91C358.25,-452.56 358.02,-443.67 357.8,-435.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"361.29,-434.93 357.54,-425.02 354.3,-435.11 361.29,-434.93\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#853fe6\" stroke=\"black\" d=\"M601,-425C601,-425 466,-425 466,-425 460,-425 454,-419 454,-413 454,-413 454,-354 454,-354 454,-348 460,-342 466,-342 466,-342 601,-342 601,-342 607,-342 613,-348 613,-354 613,-354 613,-413 613,-413 613,-419 607,-425 601,-425\"/>\n<text text-anchor=\"start\" x=\"462\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n<text text-anchor=\"start\" x=\"498\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.059</text>\n<text text-anchor=\"start\" x=\"492.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n<text text-anchor=\"start\" x=\"483\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 32]</text>\n<text text-anchor=\"start\" x=\"485\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 6&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>6&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M419.87,-460.91C434.31,-451.2 449.83,-440.76 464.63,-430.81\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"466.89,-433.51 473.24,-425.02 462.99,-427.7 466.89,-433.51\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M252,-298.5C252,-298.5 155,-298.5 155,-298.5 149,-298.5 143,-292.5 143,-286.5 143,-286.5 143,-242.5 143,-242.5 143,-236.5 149,-230.5 155,-230.5 155,-230.5 252,-230.5 252,-230.5 258,-230.5 264,-236.5 264,-242.5 264,-242.5 264,-286.5 264,-286.5 264,-292.5 258,-298.5 252,-298.5\"/>\n<text text-anchor=\"start\" x=\"175.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"166\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"156.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"151\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M303.42,-341.91C287.69,-329.88 270.5,-316.73 254.88,-304.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"256.94,-301.96 246.87,-298.67 252.69,-307.52 256.94,-301.96\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M424.5,-306C424.5,-306 294.5,-306 294.5,-306 288.5,-306 282.5,-300 282.5,-294 282.5,-294 282.5,-235 282.5,-235 282.5,-229 288.5,-223 294.5,-223 294.5,-223 424.5,-223 424.5,-223 430.5,-223 436.5,-229 436.5,-235 436.5,-235 436.5,-294 436.5,-294 436.5,-300 430.5,-306 424.5,-306\"/>\n<text text-anchor=\"start\" x=\"290.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.55</text>\n<text text-anchor=\"start\" x=\"324\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"322\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"312.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M357.54,-341.91C357.75,-333.56 357.98,-324.67 358.2,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"361.7,-316.11 358.46,-306.02 354.71,-315.93 361.7,-316.11\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M254,-179.5C254,-179.5 165,-179.5 165,-179.5 159,-179.5 153,-173.5 153,-167.5 153,-167.5 153,-123.5 153,-123.5 153,-117.5 159,-111.5 165,-111.5 165,-111.5 254,-111.5 254,-111.5 260,-111.5 266,-117.5 266,-123.5 266,-123.5 266,-167.5 266,-167.5 266,-173.5 260,-179.5 254,-179.5\"/>\n<text text-anchor=\"start\" x=\"181.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"172\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"162.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"start\" x=\"161\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M307.46,-222.91C292.18,-210.99 275.49,-197.98 260.29,-186.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"262.06,-183.06 252.02,-179.67 257.75,-188.58 262.06,-183.06\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M431,-187C431,-187 296,-187 296,-187 290,-187 284,-181 284,-175 284,-175 284,-116 284,-116 284,-110 290,-104 296,-104 296,-104 431,-104 431,-104 437,-104 443,-110 443,-116 443,-116 443,-175 443,-175 443,-181 437,-187 431,-187\"/>\n<text text-anchor=\"start\" x=\"292\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.45</text>\n<text text-anchor=\"start\" x=\"328\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"326\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"316.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M360.89,-222.91C361.17,-214.56 361.48,-205.67 361.77,-197.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"365.27,-197.13 362.11,-187.02 358.27,-196.9 365.27,-197.13\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M345,-68C345,-68 248,-68 248,-68 242,-68 236,-62 236,-56 236,-56 236,-12 236,-12 236,-6 242,0 248,0 248,0 345,0 345,0 351,0 357,-6 357,-12 357,-12 357,-56 357,-56 357,-62 351,-68 345,-68\"/>\n<text text-anchor=\"start\" x=\"268.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"259\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"249.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"244\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 11&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>11&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M338.55,-103.73C333.19,-94.97 327.52,-85.7 322.14,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"325.08,-75 316.88,-68.3 319.11,-78.66 325.08,-75\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M476,-68C476,-68 387,-68 387,-68 381,-68 375,-62 375,-56 375,-56 375,-12 375,-12 375,-6 381,0 387,0 387,0 476,0 476,0 482,0 488,-6 488,-12 488,-12 488,-56 488,-56 488,-62 482,-68 476,-68\"/>\n<text text-anchor=\"start\" x=\"403.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"394\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"384.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"383\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 11&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>11&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M388.82,-103.73C394.26,-94.97 400.01,-85.7 405.48,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"408.52,-78.64 410.82,-68.3 402.57,-74.95 408.52,-78.64\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M594,-306C594,-306 469,-306 469,-306 463,-306 457,-300 457,-294 457,-294 457,-235 457,-235 457,-229 463,-223 469,-223 469,-223 594,-223 594,-223 600,-223 606,-229 606,-235 606,-235 606,-294 606,-294 606,-300 600,-306 594,-306\"/>\n<text text-anchor=\"start\" x=\"465\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal width (cm) ≤ 3.1</text>\n<text text-anchor=\"start\" x=\"496\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"494\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"484.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n<text text-anchor=\"start\" x=\"483\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 14&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>14&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M532.81,-341.91C532.66,-333.56 532.51,-324.67 532.36,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"535.86,-315.96 532.19,-306.02 528.86,-316.08 535.86,-315.96\"/>\n</g>\n<!-- 18 -->\n<g id=\"node19\" class=\"node\">\n<title>18</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M729,-298.5C729,-298.5 636,-298.5 636,-298.5 630,-298.5 624,-292.5 624,-286.5 624,-286.5 624,-242.5 624,-242.5 624,-236.5 630,-230.5 636,-230.5 636,-230.5 729,-230.5 729,-230.5 735,-230.5 741,-236.5 741,-242.5 741,-242.5 741,-286.5 741,-286.5 741,-292.5 735,-298.5 729,-298.5\"/>\n<text text-anchor=\"start\" x=\"654.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"641.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 30</text>\n<text text-anchor=\"start\" x=\"632\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 30]</text>\n<text text-anchor=\"start\" x=\"634\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 14&#45;&gt;18 -->\n<g id=\"edge18\" class=\"edge\">\n<title>14&#45;&gt;18</title>\n<path fill=\"none\" stroke=\"black\" d=\"M585.19,-341.91C600.37,-329.99 616.95,-316.98 632.04,-305.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"634.56,-307.6 640.26,-298.67 630.24,-302.09 634.56,-307.6\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M573,-179.5C573,-179.5 484,-179.5 484,-179.5 478,-179.5 472,-173.5 472,-167.5 472,-167.5 472,-123.5 472,-123.5 472,-117.5 478,-111.5 484,-111.5 484,-111.5 573,-111.5 573,-111.5 579,-111.5 585,-117.5 585,-123.5 585,-123.5 585,-167.5 585,-167.5 585,-173.5 579,-179.5 573,-179.5\"/>\n<text text-anchor=\"start\" x=\"500.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"491\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"481.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n<text text-anchor=\"start\" x=\"480\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 15&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>15&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M530.46,-222.91C530.18,-212.2 529.89,-200.62 529.61,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"533.11,-189.57 529.35,-179.67 526.11,-189.75 533.11,-189.57\"/>\n</g>\n<!-- 17 -->\n<g id=\"node18\" class=\"node\">\n<title>17</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M712,-179.5C712,-179.5 615,-179.5 615,-179.5 609,-179.5 603,-173.5 603,-167.5 603,-167.5 603,-123.5 603,-123.5 603,-117.5 609,-111.5 615,-111.5 615,-111.5 712,-111.5 712,-111.5 718,-111.5 724,-117.5 724,-123.5 724,-123.5 724,-167.5 724,-167.5 724,-173.5 718,-179.5 712,-179.5\"/>\n<text text-anchor=\"start\" x=\"635.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"626\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"616.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"611\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 15&#45;&gt;17 -->\n<g id=\"edge17\" class=\"edge\">\n<title>15&#45;&gt;17</title>\n<path fill=\"none\" stroke=\"black\" d=\"M577.3,-222.91C590.62,-211.1 605.15,-198.22 618.43,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"620.92,-188.92 626.08,-179.67 616.28,-183.68 620.92,-188.92\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x78713c91fc10>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Decision Tree Classifier (Entropy) Visualization:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"642pt\" height=\"909pt\"\n viewBox=\"0.00 0.00 642.00 909.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 905)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-905 638,-905 638,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M265,-901C265,-901 130,-901 130,-901 124,-901 118,-895 118,-889 118,-889 118,-830 118,-830 118,-824 124,-818 130,-818 130,-818 265,-818 265,-818 271,-818 277,-824 277,-830 277,-830 277,-889 277,-889 277,-895 271,-901 265,-901\"/>\n<text text-anchor=\"start\" x=\"126\" y=\"-885.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n<text text-anchor=\"start\" x=\"153.5\" y=\"-870.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.58</text>\n<text text-anchor=\"start\" x=\"152.5\" y=\"-855.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 105</text>\n<text text-anchor=\"start\" x=\"139.5\" y=\"-840.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 37, 37]</text>\n<text text-anchor=\"start\" x=\"145\" y=\"-825.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M166,-774.5C166,-774.5 73,-774.5 73,-774.5 67,-774.5 61,-768.5 61,-762.5 61,-762.5 61,-718.5 61,-718.5 61,-712.5 67,-706.5 73,-706.5 73,-706.5 166,-706.5 166,-706.5 172,-706.5 178,-712.5 178,-718.5 178,-718.5 178,-762.5 178,-762.5 178,-768.5 172,-774.5 166,-774.5\"/>\n<text text-anchor=\"start\" x=\"79.5\" y=\"-759.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"78.5\" y=\"-744.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 31</text>\n<text text-anchor=\"start\" x=\"69\" y=\"-729.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 0, 0]</text>\n<text text-anchor=\"start\" x=\"76\" y=\"-714.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M170.44,-817.91C162.93,-806.65 154.78,-794.42 147.24,-783.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"150.07,-781.05 141.61,-774.67 144.25,-784.93 150.07,-781.05\"/>\n<text text-anchor=\"middle\" x=\"136.71\" y=\"-795.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M343,-782C343,-782 208,-782 208,-782 202,-782 196,-776 196,-770 196,-770 196,-711 196,-711 196,-705 202,-699 208,-699 208,-699 343,-699 343,-699 349,-699 355,-705 355,-711 355,-711 355,-770 355,-770 355,-776 349,-782 343,-782\"/>\n<text text-anchor=\"start\" x=\"204\" y=\"-766.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.75</text>\n<text text-anchor=\"start\" x=\"235.5\" y=\"-751.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.0</text>\n<text text-anchor=\"start\" x=\"234.5\" y=\"-736.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 74</text>\n<text text-anchor=\"start\" x=\"221\" y=\"-721.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 37, 37]</text>\n<text text-anchor=\"start\" x=\"223\" y=\"-706.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M224.56,-817.91C230.43,-809.1 236.7,-799.7 242.76,-790.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"245.85,-792.28 248.49,-782.02 240.03,-788.4 245.85,-792.28\"/>\n<text text-anchor=\"middle\" x=\"253.39\" y=\"-802.84\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#3fe685\" stroke=\"black\" d=\"M251.5,-663C251.5,-663 129.5,-663 129.5,-663 123.5,-663 117.5,-657 117.5,-651 117.5,-651 117.5,-592 117.5,-592 117.5,-586 123.5,-580 129.5,-580 129.5,-580 251.5,-580 251.5,-580 257.5,-580 263.5,-586 263.5,-592 263.5,-592 263.5,-651 263.5,-651 263.5,-657 257.5,-663 251.5,-663\"/>\n<text text-anchor=\"start\" x=\"125.5\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.6</text>\n<text text-anchor=\"start\" x=\"143\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.196</text>\n<text text-anchor=\"start\" x=\"149.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n<text text-anchor=\"start\" x=\"140\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 32, 1]</text>\n<text text-anchor=\"start\" x=\"138\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M246.01,-698.91C239.55,-690.01 232.64,-680.51 225.98,-671.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"228.65,-669.05 219.94,-663.02 222.98,-673.17 228.65,-669.05\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#9254e9\" stroke=\"black\" d=\"M429,-663C429,-663 294,-663 294,-663 288,-663 282,-657 282,-651 282,-651 282,-592 282,-592 282,-586 288,-580 294,-580 294,-580 429,-580 429,-580 435,-580 441,-586 441,-592 441,-592 441,-651 441,-651 441,-657 435,-663 429,-663\"/>\n<text text-anchor=\"start\" x=\"290\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.15</text>\n<text text-anchor=\"start\" x=\"314\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.535</text>\n<text text-anchor=\"start\" x=\"320.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 41</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 5, 36]</text>\n<text text-anchor=\"start\" x=\"313\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M305.34,-698.91C311.87,-690.01 318.86,-680.51 325.61,-671.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"328.61,-673.15 331.72,-663.02 322.97,-669.01 328.61,-673.15\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-536.5C109,-536.5 12,-536.5 12,-536.5 6,-536.5 0,-530.5 0,-524.5 0,-524.5 0,-480.5 0,-480.5 0,-474.5 6,-468.5 12,-468.5 12,-468.5 109,-468.5 109,-468.5 115,-468.5 121,-474.5 121,-480.5 121,-480.5 121,-524.5 121,-524.5 121,-530.5 115,-536.5 109,-536.5\"/>\n<text text-anchor=\"start\" x=\"20.5\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 32</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 32, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M145.4,-579.91C132.28,-568.1 117.96,-555.22 104.89,-543.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107.13,-540.76 97.35,-536.67 102.44,-545.96 107.13,-540.76\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-536.5C240,-536.5 151,-536.5 151,-536.5 145,-536.5 139,-530.5 139,-524.5 139,-524.5 139,-480.5 139,-480.5 139,-474.5 145,-468.5 151,-468.5 151,-468.5 240,-468.5 240,-468.5 246,-468.5 252,-474.5 252,-480.5 252,-480.5 252,-524.5 252,-524.5 252,-530.5 246,-536.5 240,-536.5\"/>\n<text text-anchor=\"start\" x=\"155.5\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>3&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.23,-579.91C192.69,-569.2 193.19,-557.62 193.65,-546.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"197.15,-546.81 194.08,-536.67 190.16,-546.51 197.15,-546.81\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M421.5,-544C421.5,-544 291.5,-544 291.5,-544 285.5,-544 279.5,-538 279.5,-532 279.5,-532 279.5,-473 279.5,-473 279.5,-467 285.5,-461 291.5,-461 291.5,-461 421.5,-461 421.5,-461 427.5,-461 433.5,-467 433.5,-473 433.5,-473 433.5,-532 433.5,-532 433.5,-538 427.5,-544 421.5,-544\"/>\n<text text-anchor=\"start\" x=\"287.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n<text text-anchor=\"start\" x=\"309\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.918</text>\n<text text-anchor=\"start\" x=\"315.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 15</text>\n<text text-anchor=\"start\" x=\"306\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 5, 10]</text>\n<text text-anchor=\"start\" x=\"308\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M359.77,-579.91C359.41,-571.56 359.03,-562.67 358.66,-554.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"362.16,-553.86 358.23,-544.02 355.16,-554.16 362.16,-553.86\"/>\n</g>\n<!-- 18 -->\n<g id=\"node19\" class=\"node\">\n<title>18</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M557,-536.5C557,-536.5 464,-536.5 464,-536.5 458,-536.5 452,-530.5 452,-524.5 452,-524.5 452,-480.5 452,-480.5 452,-474.5 458,-468.5 464,-468.5 464,-468.5 557,-468.5 557,-468.5 563,-468.5 569,-474.5 569,-480.5 569,-480.5 569,-524.5 569,-524.5 569,-530.5 563,-536.5 557,-536.5\"/>\n<text text-anchor=\"start\" x=\"470.5\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"469.5\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 26</text>\n<text text-anchor=\"start\" x=\"460\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 26]</text>\n<text text-anchor=\"start\" x=\"462\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 6&#45;&gt;18 -->\n<g id=\"edge18\" class=\"edge\">\n<title>6&#45;&gt;18</title>\n<path fill=\"none\" stroke=\"black\" d=\"M413.19,-579.91C428.37,-567.99 444.95,-554.98 460.04,-543.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"462.56,-545.6 468.26,-536.67 458.24,-540.09 462.56,-545.6\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M338,-425C338,-425 205,-425 205,-425 199,-425 193,-419 193,-413 193,-413 193,-354 193,-354 193,-348 199,-342 205,-342 205,-342 338,-342 338,-342 344,-342 350,-348 350,-354 350,-354 350,-413 350,-413 350,-419 344,-425 338,-425\"/>\n<text text-anchor=\"start\" x=\"201\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal width (cm) ≤ 2.35</text>\n<text text-anchor=\"start\" x=\"224\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.918</text>\n<text text-anchor=\"start\" x=\"234\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"224.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 4, 2]</text>\n<text text-anchor=\"start\" x=\"219\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M327.01,-460.91C320.55,-452.01 313.64,-442.51 306.98,-433.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"309.65,-431.05 300.94,-425.02 303.98,-435.17 309.65,-431.05\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#9152e8\" stroke=\"black\" d=\"M505,-425C505,-425 380,-425 380,-425 374,-425 368,-419 368,-413 368,-413 368,-354 368,-354 368,-348 374,-342 380,-342 380,-342 505,-342 505,-342 511,-342 517,-348 517,-354 517,-354 517,-413 517,-413 517,-419 511,-425 505,-425\"/>\n<text text-anchor=\"start\" x=\"376\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal width (cm) ≤ 3.1</text>\n<text text-anchor=\"start\" x=\"395\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.503</text>\n<text text-anchor=\"start\" x=\"405\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 9</text>\n<text text-anchor=\"start\" x=\"395.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 8]</text>\n<text text-anchor=\"start\" x=\"394\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>7&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M386.34,-460.91C392.87,-452.01 399.86,-442.51 406.61,-433.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"409.61,-435.15 412.72,-425.02 403.97,-431.01 409.61,-435.15\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M167,-298.5C167,-298.5 78,-298.5 78,-298.5 72,-298.5 66,-292.5 66,-286.5 66,-286.5 66,-242.5 66,-242.5 66,-236.5 72,-230.5 78,-230.5 78,-230.5 167,-230.5 167,-230.5 173,-230.5 179,-236.5 179,-242.5 179,-242.5 179,-286.5 179,-286.5 179,-292.5 173,-298.5 167,-298.5\"/>\n<text text-anchor=\"start\" x=\"82.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"85\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"75.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"74\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 8&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>8&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M219.81,-341.91C204.63,-329.99 188.05,-316.98 172.96,-305.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"174.76,-302.09 164.74,-298.67 170.44,-307.6 174.76,-302.09\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#6aeca0\" stroke=\"black\" d=\"M344,-306C344,-306 209,-306 209,-306 203,-306 197,-300 197,-294 197,-294 197,-235 197,-235 197,-229 203,-223 209,-223 209,-223 344,-223 344,-223 350,-223 356,-229 356,-235 356,-235 356,-294 356,-294 356,-300 350,-306 344,-306\"/>\n<text text-anchor=\"start\" x=\"205\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.05</text>\n<text text-anchor=\"start\" x=\"229\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.722</text>\n<text text-anchor=\"start\" x=\"239\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\n<text text-anchor=\"start\" x=\"229.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 4, 1]</text>\n<text text-anchor=\"start\" x=\"224\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 8&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>8&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M273.23,-341.91C273.59,-333.56 273.97,-324.67 274.34,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"277.84,-316.16 274.77,-306.02 270.84,-315.86 277.84,-316.16\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M245,-179.5C245,-179.5 148,-179.5 148,-179.5 142,-179.5 136,-173.5 136,-167.5 136,-167.5 136,-123.5 136,-123.5 136,-117.5 142,-111.5 148,-111.5 148,-111.5 245,-111.5 245,-111.5 251,-111.5 257,-117.5 257,-123.5 257,-123.5 257,-167.5 257,-167.5 257,-173.5 251,-179.5 245,-179.5\"/>\n<text text-anchor=\"start\" x=\"156.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"159\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"149.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 3, 0]</text>\n<text text-anchor=\"start\" x=\"144\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 10&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>10&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M248.74,-222.91C241.05,-211.65 232.68,-199.42 224.95,-188.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"227.71,-185.95 219.18,-179.67 221.93,-189.9 227.71,-185.95\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M425.5,-187C425.5,-187 287.5,-187 287.5,-187 281.5,-187 275.5,-181 275.5,-175 275.5,-175 275.5,-116 275.5,-116 275.5,-110 281.5,-104 287.5,-104 287.5,-104 425.5,-104 425.5,-104 431.5,-104 437.5,-110 437.5,-116 437.5,-116 437.5,-175 437.5,-175 437.5,-181 431.5,-187 425.5,-187\"/>\n<text text-anchor=\"start\" x=\"283.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal length (cm) ≤ 6.15</text>\n<text text-anchor=\"start\" x=\"316.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.0</text>\n<text text-anchor=\"start\" x=\"319\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"309.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 1]</text>\n<text text-anchor=\"start\" x=\"304\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 10&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>10&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M304.26,-222.91C310.34,-214.01 316.84,-204.51 323.11,-195.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"326.04,-197.25 328.79,-187.02 320.26,-193.3 326.04,-197.25\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M337,-68C337,-68 240,-68 240,-68 234,-68 228,-62 228,-56 228,-56 228,-12 228,-12 228,-6 234,0 240,0 240,0 337,0 337,0 343,0 349,-6 349,-12 349,-12 349,-56 349,-56 349,-62 343,-68 337,-68\"/>\n<text text-anchor=\"start\" x=\"248.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"251\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"241.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"236\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 12&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>12&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M331.18,-103.73C325.74,-94.97 319.99,-85.7 314.52,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"317.43,-74.95 309.18,-68.3 311.48,-78.64 317.43,-74.95\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M468,-68C468,-68 379,-68 379,-68 373,-68 367,-62 367,-56 367,-56 367,-12 367,-12 367,-6 373,0 379,0 379,0 468,0 468,0 474,0 480,-6 480,-12 480,-12 480,-56 480,-56 480,-62 474,-68 468,-68\"/>\n<text text-anchor=\"start\" x=\"383.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"386\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"376.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"375\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>12&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M381.45,-103.73C386.81,-94.97 392.48,-85.7 397.86,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"400.89,-78.66 403.12,-68.3 394.92,-75 400.89,-78.66\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M483,-298.5C483,-298.5 394,-298.5 394,-298.5 388,-298.5 382,-292.5 382,-286.5 382,-286.5 382,-242.5 382,-242.5 382,-236.5 388,-230.5 394,-230.5 394,-230.5 483,-230.5 483,-230.5 489,-230.5 495,-236.5 495,-242.5 495,-242.5 495,-286.5 495,-286.5 495,-292.5 489,-298.5 483,-298.5\"/>\n<text text-anchor=\"start\" x=\"398.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"401\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8</text>\n<text text-anchor=\"start\" x=\"391.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 8]</text>\n<text text-anchor=\"start\" x=\"390\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 15&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>15&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M441.11,-341.91C440.75,-331.2 440.35,-319.62 439.98,-308.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"443.47,-308.54 439.63,-298.67 436.48,-308.78 443.47,-308.54\"/>\n</g>\n<!-- 17 -->\n<g id=\"node18\" class=\"node\">\n<title>17</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M622,-298.5C622,-298.5 525,-298.5 525,-298.5 519,-298.5 513,-292.5 513,-286.5 513,-286.5 513,-242.5 513,-242.5 513,-236.5 519,-230.5 525,-230.5 525,-230.5 622,-230.5 622,-230.5 628,-230.5 634,-236.5 634,-242.5 634,-242.5 634,-286.5 634,-286.5 634,-292.5 628,-298.5 622,-298.5\"/>\n<text text-anchor=\"start\" x=\"533.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"536\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"526.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"521\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 15&#45;&gt;17 -->\n<g id=\"edge17\" class=\"edge\">\n<title>15&#45;&gt;17</title>\n<path fill=\"none\" stroke=\"black\" d=\"M487.95,-341.91C501.17,-330.1 515.6,-317.22 528.77,-305.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"531.24,-307.94 536.36,-298.67 526.57,-302.72 531.24,-307.94\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x78711d395d50>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "21. # Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "# accuracy with a fully grown tree\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "clf_limited_depth = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited_depth.fit(X_train, y_train)\n",
        "y_pred_limited_depth = clf_limited_depth.predict(X_test)\n",
        "accuracy_limited_depth = accuracy_score(y_test, y_pred_limited_depth)\n",
        "print(\"Accuracy (Max Depth 3):\", accuracy_limited_depth)\n",
        "\n",
        "# Train a fully grown Decision Tree Classifier (default max_depth=None)\n",
        "clf_full_depth = DecisionTreeClassifier(random_state=42)\n",
        "clf_full_depth.fit(X_train, y_train)\n",
        "y_pred_full_depth = clf_full_depth.predict(X_test)\n",
        "accuracy_full_depth = accuracy_score(y_test, y_pred_full_depth)\n",
        "print(\"Accuracy (Full Depth):\", accuracy_full_depth)\n",
        "\n",
        "# Compare the accuracies\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited_depth:.4f}\")\n",
        "print(f\"Accuracy with full depth: {accuracy_full_depth:.4f}\")\n",
        "\n",
        "if accuracy_full_depth > accuracy_limited_depth:\n",
        "    print(\"The fully grown tree has higher accuracy on the test set.\")\n",
        "elif accuracy_limited_depth > accuracy_full_depth:\n",
        "    print(\"The tree with max_depth=3 has higher accuracy on the test set.\")\n",
        "else:\n",
        "    print(\"Both trees have the same accuracy on the test set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw65P7zyF2Z2",
        "outputId": "e51c6464-abbc-482c-e072-49fd4261cc3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Max Depth 3): 1.0\n",
            "Accuracy (Full Depth): 1.0\n",
            "\n",
            "Comparison:\n",
            "Accuracy with max_depth=3: 1.0000\n",
            "Accuracy with full depth: 1.0000\n",
            "Both trees have the same accuracy on the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "22. #Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "# accuracy with a default tree\n",
        "\n",
        "# Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "# accuracy with a default tree\n",
        "\n",
        "# Train a Decision Tree Classifier with min_samples_split=5\n",
        "clf_min_samples_split = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_min_samples_split.fit(X_train, y_train)\n",
        "y_pred_min_samples_split = clf_min_samples_split.predict(X_test)\n",
        "accuracy_min_samples_split = accuracy_score(y_test, y_pred_min_samples_split)\n",
        "print(\"Accuracy (min_samples_split=5):\", accuracy_min_samples_split)\n",
        "\n",
        "# Train a default Decision Tree Classifier\n",
        "# A default tree in scikit-learn has min_samples_split=2\n",
        "clf_default = DecisionTreeClassifier(random_state=42) # Default min_samples_split is 2\n",
        "clf_default.fit(X_train, y_train)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "print(\"Accuracy (Default Tree):\", accuracy_default)\n",
        "\n",
        "# Compare the accuracies\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Accuracy with min_samples_split=5: {accuracy_min_samples_split:.4f}\")\n",
        "print(f\"Accuracy with default tree: {accuracy_default:.4f}\")\n",
        "\n",
        "if accuracy_default > accuracy_min_samples_split:\n",
        "    print(\"The default tree has higher accuracy on the test set.\")\n",
        "elif accuracy_min_samples_split > accuracy_default:\n",
        "    print(\"The tree with min_samples_split=5 has higher accuracy on the test set.\")\n",
        "else:\n",
        "    print(\"Both trees have the same accuracy on the test set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNd2QeZTGDKk",
        "outputId": "0f43d22a-6a2c-4d6a-9198-6a1ae7eceb1e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (min_samples_split=5): 1.0\n",
            "Accuracy (Default Tree): 1.0\n",
            "\n",
            "Comparison:\n",
            "Accuracy with min_samples_split=5: 1.0000\n",
            "Accuracy with default tree: 1.0000\n",
            "Both trees have the same accuracy on the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "23. #Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "# accuracy with unscaled data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a Decision Tree Classifier on scaled data\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy (Scaled Data):\", accuracy_scaled)\n",
        "\n",
        "# Train a Decision Tree Classifier on unscaled data (already done in previous cells, but doing it here for direct comparison)\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(\"Accuracy (Unscaled Data):\", accuracy_unscaled)\n",
        "\n",
        "# Compare the accuracies\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Accuracy with scaled data: {accuracy_scaled:.4f}\")\n",
        "print(f\"Accuracy with unscaled data: {accuracy_unscaled:.4f}\")\n",
        "\n",
        "# Explain why feature scaling might not significantly impact Decision Trees (or might, depending on the dataset)\n",
        "print(\"\\nObservation:\")\n",
        "print(\"Feature scaling generally has little to no impact on the performance of Decision Trees.\")\n",
        "print(\"This is because Decision Trees are non-parametric models that make decisions based on thresholds of individual features,\")\n",
        "print(\"not on the distances or magnitudes between data points.\")\n",
        "print(\"Splitting criteria (like Gini impurity or entropy) are based on the proportions of classes within a node,\")\n",
        "print(\"which are not affected by the scaling of the features.\")\n",
        "print(\"The threshold for a split will simply adjust to the new scale of the feature.\")\n",
        "print(\"However, for ensemble methods based on Decision Trees (like some boosting algorithms that use gradient descent),\")\n",
        "print(\"scaling *might* have an indirect effect on convergence speed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSluzX00GP3D",
        "outputId": "b2b018e6-c2ba-4f8a-f384-31082e850557"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Scaled Data): 1.0\n",
            "Accuracy (Unscaled Data): 1.0\n",
            "\n",
            "Comparison:\n",
            "Accuracy with scaled data: 1.0000\n",
            "Accuracy with unscaled data: 1.0000\n",
            "\n",
            "Observation:\n",
            "Feature scaling generally has little to no impact on the performance of Decision Trees.\n",
            "This is because Decision Trees are non-parametric models that make decisions based on thresholds of individual features,\n",
            "not on the distances or magnitudes between data points.\n",
            "Splitting criteria (like Gini impurity or entropy) are based on the proportions of classes within a node,\n",
            "which are not affected by the scaling of the features.\n",
            "The threshold for a split will simply adjust to the new scale of the feature.\n",
            "However, for ensemble methods based on Decision Trees (like some boosting algorithms that use gradient descent),\n",
            "scaling *might* have an indirect effect on convergence speed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "24. #Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "# classification\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Create a One-vs-Rest Classifier with a Decision Tree base estimator\n",
        "ovr_clf = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "\n",
        "# Train the One-vs-Rest Classifier\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the response for test dataset\n",
        "y_pred_ovr = ovr_clf.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(\"Accuracy (One-vs-Rest Decision Tree):\", accuracy_ovr)\n",
        "\n",
        "# You can also examine the individual classifiers trained for each class\n",
        "print(\"\\nNumber of estimators in One-vs-Rest Classifier:\", len(ovr_clf.estimators_))\n",
        "# For a dataset with 3 classes like Iris, there will be 3 base estimators."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2AR8l1qGbAT",
        "outputId": "e869f6eb-0a64-495f-c517-e61d7bfc8427"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (One-vs-Rest Decision Tree): 1.0\n",
            "\n",
            "Number of estimators in One-vs-Rest Classifier: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "25. #Write a Python program to train a Decision Tree Classifier and display the feature importance scores\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importance Scores:\")\n",
        "for feature, importance in zip(iris.feature_names, feature_importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv03h9KfGnRR",
        "outputId": "ec368822-bd78-4775-e197-448170306a41"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance Scores:\n",
            "sepal length (cm): 0.0191\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.4045\n",
            "petal width (cm): 0.5573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "26. #Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "# with an unrestricted tree\n",
        "\n",
        "#Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "#with an unrestricted tree\n",
        "\n",
        "# Train a Decision Tree Regressor with max_depth=5\n",
        "regressor_limited_depth = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "regressor_limited_depth.fit(X_train_reg, y_train_reg)\n",
        "y_pred_limited_depth_reg = regressor_limited_depth.predict(X_test_reg)\n",
        "mse_limited_depth = mean_squared_error(y_test_reg, y_pred_limited_depth_reg)\n",
        "print(\"Mean Squared Error (Max Depth 5):\", mse_limited_depth)\n",
        "\n",
        "# Train an unrestricted Decision Tree Regressor (default max_depth=None)\n",
        "regressor_full_depth = DecisionTreeRegressor(random_state=42)\n",
        "regressor_full_depth.fit(X_train_reg, y_train_reg)\n",
        "y_pred_full_depth_reg = regressor_full_depth.predict(X_test_reg)\n",
        "mse_full_depth = mean_squared_error(y_test_reg, y_pred_full_depth_reg)\n",
        "print(\"Mean Squared Error (Unrestricted Tree):\", mse_full_depth)\n",
        "\n",
        "# Compare the performance (lower MSE is better)\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"MSE with max_depth=5: {mse_limited_depth:.4f}\")\n",
        "print(f\"MSE with unrestricted tree: {mse_full_depth:.4f}\")\n",
        "\n",
        "if mse_full_depth < mse_limited_depth:\n",
        "    print(\"The unrestricted tree has a lower MSE on the test set (better performance).\")\n",
        "elif mse_limited_depth < mse_full_depth:\n",
        "    print(\"The tree with max_depth=5 has a lower MSE on the test set (better performance).\")\n",
        "else:\n",
        "    print(\"Both trees have the same MSE on the test set.\")\n",
        "\n",
        "print(\"\\nObservation:\")\n",
        "print(\"An unrestricted Decision Tree Regressor is prone to overfitting the training data,\")\n",
        "print(\"leading to potentially lower performance on unseen data compared to a tree with limited depth.\")\n",
        "print(\"The tree with max_depth=5 acts as a form of pre-pruning, which can help prevent overfitting.\")\n",
        "print(\"In this specific case, you can see which model performed better based on the MSE values.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxGYdWcEGzQ8",
        "outputId": "9a2782a5-943d-495a-eaa7-db86655cfd27"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Max Depth 5): 0.5210801561811793\n",
            "Mean Squared Error (Unrestricted Tree): 0.5280096503174904\n",
            "\n",
            "Comparison:\n",
            "MSE with max_depth=5: 0.5211\n",
            "MSE with unrestricted tree: 0.5280\n",
            "The tree with max_depth=5 has a lower MSE on the test set (better performance).\n",
            "\n",
            "Observation:\n",
            "An unrestricted Decision Tree Regressor is prone to overfitting the training data,\n",
            "leading to potentially lower performance on unseen data compared to a tree with limited depth.\n",
            "The tree with max_depth=5 acts as a form of pre-pruning, which can help prevent overfitting.\n",
            "In this specific case, you can see which model performed better based on the MSE values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "27.  #Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "# visualize its effect on accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Train a Decision Tree Classifier on the Iris dataset\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Calculate the cost complexity pruning paths\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Store the accuracy for each alpha\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf_pruned.fit(X_train, y_train)\n",
        "    clfs.append(clf_pruned)\n",
        "\n",
        "# Remove the last element because it is the trivial tree with only one node\n",
        "clfs = clfs[:-1]\n",
        "ccp_alphas = ccp_alphas[:-1]\n",
        "\n",
        "# Calculate accuracy on the test set for each pruned tree\n",
        "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
        "\n",
        "# Visualize the effect of ccp_alpha on accuracy\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(ccp_alphas, test_scores, marker='o', drawstyle=\"steps-post\")\n",
        "ax.set_xlabel(\"effective alpha\")\n",
        "ax.set_ylabel(\"accuracy on test set\")\n",
        "ax.set_title(\"Accuracy vs. effective alpha for training and test sets\")\n",
        "plt.show()\n",
        "\n",
        "# Find the best alpha based on test accuracy\n",
        "best_alpha_index = test_scores.index(max(test_scores))\n",
        "best_alpha = ccp_alphas[best_alpha_index]\n",
        "print(f\"\\nOptimal ccp_alpha based on test accuracy: {best_alpha:.4f}\")\n",
        "\n",
        "# Train the final model with the optimal alpha\n",
        "final_clf = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)\n",
        "final_clf.fit(X_train, y_train)\n",
        "final_accuracy = accuracy_score(y_test, final_clf.predict(X_test))\n",
        "print(f\"Accuracy of the pruned tree with optimal alpha: {final_accuracy:.4f}\")\n",
        "\n",
        "# Compare with the accuracy of the unpruned tree\n",
        "unpruned_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
        "print(f\"Accuracy of the unpruned tree: {unpruned_accuracy:.4f}\")\n",
        "\n",
        "# Visualize the unpruned tree (optional, can be large)\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
        "# plt.title(\"Unpruned Decision Tree\")\n",
        "# plt.show()\n",
        "\n",
        "# Visualize the pruned tree (optional, will be smaller)\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# plot_tree(final_clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
        "# plt.title(f\"Pruned Decision Tree (ccp_alpha={best_alpha:.4f})\")\n",
        "# plt.show()\n",
        "\n",
        "print(\"\\nEffect of Cost Complexity Pruning:\")\n",
        "print(\"Cost Complexity Pruning aims to find a balance between the complexity of the tree (number of nodes)\")\n",
        "print(\"and its ability to generalize to unseen data. It does this by adding a penalty term (ccp_alpha)\")\n",
        "print(\"to the impurity measure during the pruning process.\")\n",
        "print(\"Higher values of ccp_alpha result in more pruning (simpler trees).\")\n",
        "print(\"By plotting the accuracy on the test set against different alpha values, we can identify the alpha\")\n",
        "print(\"that yields the best performance on unseen data, which helps in selecting the right level of pruning.\")\n",
        "print(\"In this example, we can see how the test accuracy changes as the tree is progressively pruned.\")\n",
        "print(\"The optimal alpha corresponds to the point where the test accuracy is maximized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "cFq2z5PhHAXP",
        "outputId": "d26b3a77-4fcd-4815-bb1f-a42a314c5d86"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimal ccp_alpha based on test accuracy: 0.0000\n",
            "Accuracy of the pruned tree with optimal alpha: 1.0000\n",
            "Accuracy of the unpruned tree: 1.0000\n",
            "\n",
            "Effect of Cost Complexity Pruning:\n",
            "Cost Complexity Pruning aims to find a balance between the complexity of the tree (number of nodes)\n",
            "and its ability to generalize to unseen data. It does this by adding a penalty term (ccp_alpha)\n",
            "to the impurity measure during the pruning process.\n",
            "Higher values of ccp_alpha result in more pruning (simpler trees).\n",
            "By plotting the accuracy on the test set against different alpha values, we can identify the alpha\n",
            "that yields the best performance on unseen data, which helps in selecting the right level of pruning.\n",
            "In this example, we can see how the test accuracy changes as the tree is progressively pruned.\n",
            "The optimal alpha corresponds to the point where the test accuracy is maximized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "28. #Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "# Recall, and F1-Score\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Train the Decision Tree Classifier (using the clf trained previously)\n",
        "# clf = DecisionTreeClassifier(random_state=42)\n",
        "# clf.fit(X_train, y_train) # assuming X_train, y_train are already defined\n",
        "\n",
        "# Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "precision = precision_score(y_test, y_pred, average='weighted') # Use 'weighted' for multiclass\n",
        "recall = recall_score(y_test, y_pred, average='weighted')     # Use 'weighted' for multiclass\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')           # Use 'weighted' for multiclass\n",
        "\n",
        "print(\"Model Performance Evaluation:\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2_nnrHgHPH_",
        "outputId": "b32ca8d3-f459-48a5-dd58-cbe3ab8a42e7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance Evaluation:\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "29.   #Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming the Decision Tree Classifier 'clf' has been trained previously\n",
        "# and 'X_test', 'y_test', and 'y_pred' are available.\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nExplanation of the Confusion Matrix:\")\n",
        "print(\"- Each row represents the instances in an actual class.\")\n",
        "print(\"- Each column represents the instances in a predicted class.\")\n",
        "print(\"- The diagonal elements show the number of correctly classified instances for each class.\")\n",
        "print(\"- Off-diagonal elements show the number of misclassified instances.\")\n",
        "print(\"- For example, the value in row i and column j indicates the number of instances that are actually from class i but were predicted as class j.\")\n",
        "print(\"\\nA good classifier will have high values on the diagonal and low values off the diagonal.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "VdkMxiryHcWI",
        "outputId": "1098bf09-3bc6-4f78-eaed-a562f7c3b39f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation of the Confusion Matrix:\n",
            "- Each row represents the instances in an actual class.\n",
            "- Each column represents the instances in a predicted class.\n",
            "- The diagonal elements show the number of correctly classified instances for each class.\n",
            "- Off-diagonal elements show the number of misclassified instances.\n",
            "- For example, the value in row i and column j indicates the number of instances that are actually from class i but were predicted as class j.\n",
            "\n",
            "A good classifier will have high values on the diagonal and low values off the diagonal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "30.  # Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "# for max_depth and min_samples_split\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [None, 3, 5, 10], # Including None (no limit) and various depths\n",
        "    'min_samples_split': [2, 5, 10] # Various minimum samples split values\n",
        "}\n",
        "\n",
        "# Create a Decision Tree Classifier object\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "# cv=5 means 5-fold cross-validation\n",
        "grid_search = GridSearchCV(dt_clf, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Print the best accuracy score found during cross-validation\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Get the best model\n",
        "best_dt_clf = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred_best = best_dt_clf.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "print(\"Test set accuracy with best parameters:\", test_accuracy)\n",
        "\n",
        "print(\"\\nExplanation of GridSearchCV:\")\n",
        "print(\"GridSearchCV systematically searches over a specified range of hyperparameter values (param_grid).\")\n",
        "print(\"For each combination of hyperparameters, it trains and evaluates the model using cross-validation (cv=5).\")\n",
        "print(\"It keeps track of the combination of hyperparameters that results in the best cross-validation score (best_score_).\")\n",
        "print(\"Finally, it refits the model on the entire training set using the best hyperparameters found (best_estimator_).\")\n",
        "print(\"This process helps in finding the optimal hyperparameters for the model that generalize well to unseen data,\")\n",
        "print(\"reducing the risk of overfitting that might occur from training on a single train-test split.\")\n",
        "print(\"In this case, we are finding the optimal max_depth and min_samples_split for the Decision Tree Classifier.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIpcfYplHpyP",
        "outputId": "29e65c3b-eb06-4824-9f38-92cfe824d23a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': None, 'min_samples_split': 10}\n",
            "Best cross-validation accuracy: 0.9428571428571428\n",
            "Test set accuracy with best parameters: 1.0\n",
            "\n",
            "Explanation of GridSearchCV:\n",
            "GridSearchCV systematically searches over a specified range of hyperparameter values (param_grid).\n",
            "For each combination of hyperparameters, it trains and evaluates the model using cross-validation (cv=5).\n",
            "It keeps track of the combination of hyperparameters that results in the best cross-validation score (best_score_).\n",
            "Finally, it refits the model on the entire training set using the best hyperparameters found (best_estimator_).\n",
            "This process helps in finding the optimal hyperparameters for the model that generalize well to unseen data,\n",
            "reducing the risk of overfitting that might occur from training on a single train-test split.\n",
            "In this case, we are finding the optimal max_depth and min_samples_split for the Decision Tree Classifier.\n"
          ]
        }
      ]
    }
  ]
}